{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61e6e45d6e2f46f29f6de48b3579877f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d7b8211ab5354670934d9a5f13919dec"
          }
        },
        "280c13b519de4ac2988524142a9161f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564b19fbc43244e2a5cf0c089ff57073",
            "placeholder": "​",
            "style": "IPY_MODEL_08aa312c7b4449e4aadc3ca40f57beb4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "62c0f3b92d854f1f881bc2d7eecfa6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_66491c8b18d745bcad2681d27faa6536",
            "placeholder": "​",
            "style": "IPY_MODEL_be22bb25ce924130b2ce2954e528d07e",
            "value": ""
          }
        },
        "217be34f27034f1d814726c29558c10e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_69fdc0a711b34ca4a97784aab71a48d9",
            "style": "IPY_MODEL_cdb9fa7880e5481ea5d1b80f0d123f15",
            "value": true
          }
        },
        "273ed610712949a2a55e8dc7cd58eb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8b86e2d29caf475099825f8acdb9e51a",
            "style": "IPY_MODEL_73f83cca7f784bb283e40b00c6ec698e",
            "tooltip": ""
          }
        },
        "430b2b595e434b11a539166694de4d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c8eb77c89f4ca79f6fd25e6a2023da",
            "placeholder": "​",
            "style": "IPY_MODEL_ba32a950ff3c49909706f777a9ff08d7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d7b8211ab5354670934d9a5f13919dec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "564b19fbc43244e2a5cf0c089ff57073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08aa312c7b4449e4aadc3ca40f57beb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66491c8b18d745bcad2681d27faa6536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be22bb25ce924130b2ce2954e528d07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69fdc0a711b34ca4a97784aab71a48d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb9fa7880e5481ea5d1b80f0d123f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b86e2d29caf475099825f8acdb9e51a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f83cca7f784bb283e40b00c6ec698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d2c8eb77c89f4ca79f6fd25e6a2023da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba32a950ff3c49909706f777a9ff08d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ae015725a224b6bb1f9e69f2970fc17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3796d8e6fcce43999d966eb0e6aeb326",
            "placeholder": "​",
            "style": "IPY_MODEL_20dae179fec3449b9c0a2000e3610d1c",
            "value": "Connecting..."
          }
        },
        "3796d8e6fcce43999d966eb0e6aeb326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20dae179fec3449b9c0a2000e3610d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q&A with BERT-Finetuned"
      ],
      "metadata": {
        "id": "fhZy9p42zjDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы проведем дообучение BERT-модели на датасете [SQuAD1.0](https://rajpurkar.github.io/SQuAD-explorer/), состоящем из вопросов, заданных краудворкерами по набору статей Википедии.\n",
        "\n",
        "Модели, основанные только на энкодере, такие как BERT, как правило, отлично справляются с извлечением ответов на фактоидные вопросы типа \"Кто изобрел архитектуру трансформера?\", но плохо справляются с открытыми вопросами типа \"Почему небо голубое?\". В таких сложных случаях для синтеза информации обычно используются модели энкодеров-декодеров, такие как T5 и BART"
      ],
      "metadata": {
        "id": "TPKWDGUVz0B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка библиотек"
      ],
      "metadata": {
        "id": "sB2EejiG09Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "BqPVQ3I-T1V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "zRid67bLT2eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"nesterenkoms2001@gmail.com\"\n",
        "!git config --global user.name \"MaksimNS01\""
      ],
      "metadata": {
        "id": "OdMIx85_SEU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек и модулей"
      ],
      "metadata": {
        "id": "tSkh5DBP0cNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from accelerate import Accelerator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "JLr29Sga0geh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Авторизация в HuggingFace"
      ],
      "metadata": {
        "id": "BNH6cga_6IrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "61e6e45d6e2f46f29f6de48b3579877f",
            "280c13b519de4ac2988524142a9161f2",
            "62c0f3b92d854f1f881bc2d7eecfa6c7",
            "217be34f27034f1d814726c29558c10e",
            "273ed610712949a2a55e8dc7cd58eb22",
            "430b2b595e434b11a539166694de4d16",
            "d7b8211ab5354670934d9a5f13919dec",
            "564b19fbc43244e2a5cf0c089ff57073",
            "08aa312c7b4449e4aadc3ca40f57beb4",
            "66491c8b18d745bcad2681d27faa6536",
            "be22bb25ce924130b2ce2954e528d07e",
            "69fdc0a711b34ca4a97784aab71a48d9",
            "cdb9fa7880e5481ea5d1b80f0d123f15",
            "8b86e2d29caf475099825f8acdb9e51a",
            "73f83cca7f784bb283e40b00c6ec698e",
            "d2c8eb77c89f4ca79f6fd25e6a2023da",
            "ba32a950ff3c49909706f777a9ff08d7",
            "3ae015725a224b6bb1f9e69f2970fc17",
            "3796d8e6fcce43999d966eb0e6aeb326",
            "20dae179fec3449b9c0a2000e3610d1c"
          ]
        },
        "id": "wIjrXrFj1GoX",
        "outputId": "12bcaa10-e051-4ccd-bb08-ed0e2e1068e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61e6e45d6e2f46f29f6de48b3579877f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка и анализ датасета SQuAD"
      ],
      "metadata": {
        "id": "WP959Dx20MFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"squad\")"
      ],
      "metadata": {
        "id": "xAdvJlMscuzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdnTJbHwyNVb",
        "outputId": "c7983c93-c8fc-416b-e080-5d8d5c5f3047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Похоже, у нас есть все необходимое в полях context, question и answers, так что давайте выведем их для первого элемента нашего обучающего набора:"
      ],
      "metadata": {
        "id": "iLWEh6s30ny5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
        "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
        "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAJMHmdF0oMF",
        "outputId": "77768fa1-7a42-42a0-9d9f-d1eb903fa65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поля context и question очень просты в использовании. С полем answers немного сложнее, поскольку оно представляет собой словарь с двумя полями, которые оба являются списками. Именно такой формат будет ожидать метрика squad при оценке. Поле text довольно очевидно, а поле answer_start содержит индекс начального символа каждого ответа в контексте.\n",
        "\n",
        "Во время обучения существует только один возможный ответ. Мы можем перепроверить это с помощью метода Dataset.filter():"
      ],
      "metadata": {
        "id": "7UCwId3F1Xhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ],
      "metadata": {
        "id": "Cs_zdxLdcy9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для оценки, однако, существует несколько возможных ответов для каждого примера, которые могут быть одинаковыми или разными:"
      ],
      "metadata": {
        "id": "UrYWhIvK1ht5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"validation\"][0][\"answers\"])\n",
        "print(raw_datasets[\"validation\"][2][\"answers\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t7TMGpB1jNW",
        "outputId": "e56ba097-0430-4b75-d4e0-5b541e4c8c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n",
            "{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Суть в том, что некоторые вопросы имеют несколько возможных ответов, и этот сценарий будет сравнивать спрогнозированный ответ со всеми допустимыми ответами и выбирать лучший результат. Если мы посмотрим, например, на выборку с индексом 2:"
      ],
      "metadata": {
        "id": "gLW1OnQN1obV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"validation\"][2][\"context\"])\n",
        "print(raw_datasets[\"validation\"][2][\"question\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLuSmciw1ptp",
        "outputId": "056758e8-3fa0-47cb-924f-4cefa7134b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "Where did Super Bowl 50 take place?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ответ действительно может быть одним из трех возможных вариантов, которые мы видели ранее."
      ],
      "metadata": {
        "id": "boQ_BjUD1tAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка обучающих данных"
      ],
      "metadata": {
        "id": "nDSS98zb1u5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала нам нужно преобразовать текст во входных данных в идентификаторы, которые модель сможет понять, используя токенизатор:"
      ],
      "metadata": {
        "id": "iu_NZliy12gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "B4m78y2nc00o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы проверить, что используемый объект tokenizer действительно поддерживается используется его атрибут is_fast:"
      ],
      "metadata": {
        "id": "xNjkfEaD1_gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzaasCOL1_Ep",
        "outputId": "cdb45c2b-27c7-4ae1-f616-24ee3e5e0987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы можем передать нашему токенизатору вопрос и контекст вместе, и он правильно вставит специальные токены, чтобы сформировать предложение, подобное этому:"
      ],
      "metadata": {
        "id": "cD6dT6SK2G8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = raw_datasets[\"train\"][0][\"context\"]\n",
        "question = raw_datasets[\"train\"][0][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "tokenizer.decode(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ORw-ragU2GZ1",
        "outputId": "d7e28693-3a35-45c9-e13c-9a1c33ab7d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building \\' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном случае контекст не слишком длинный, но некоторые примеры в датасете имеют очень длинные контексты, которые превысят установленную нами максимальную длину (которая в данном случае равна 384).\n",
        "\n",
        "Чтобы увидеть, как это работает на текущем примере, мы можем ограничить длину до 100 и использовать скользящее окно из 50 токенов. Используем:\n",
        "* max_length (возьмем 100)\n",
        "* truncation=\"only_second\" для усечения контекста (который находится во второй позиции), когда вопрос с его контекстом слишком длинный\n",
        "* stride для задания количества перекрывающихся токенов между двумя последовательными фрагментами (возьмем 50)\n",
        "* return_overflowing_tokens=True, чтобы сообщить токенизатору, что нам нужны переполненные токены (overflowing tokens)"
      ],
      "metadata": {
        "id": "Hauze8Ae2aNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PDOPD762May",
        "outputId": "56177e26-df22-4490-8964-9f497521f492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
            "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы можем видеть, наш пример был разбит на четыре входа, каждый из которых содержит вопрос и часть контекста. Ответ на вопрос (\"Bernadette Soubirous\") появляется только в третьем и последнем входе, поэтому, работая с длинными контекстами таким образом, мы создадим несколько обучающих примеров, в которых ответ не будет включен в контекст. Для этих примеров метками будут start_position = end_position = 0 (таким образом мы предсказываем токен [CLS]). Мы также зададим эти метки в неудачном случае, когда ответ был усечен, так что у нас есть только его начало (или конец). Для примеров, где ответ полностью находится в контексте, метками будут индекс токена, с которого начинается ответ, и индекс токена, на котором ответ заканчивается.\n",
        "\n",
        "Датасет предоставляет нам начальный символ ответа в контексте, а прибавив к нему длину ответа, мы можем найти конечный символ в контексте. Чтобы сопоставить их с индексами токенов, нам нужно использовать сопоставление смещений6. Мы можем настроить наш токенизатор на их возврат, передав return_offsets_mapping=True:"
      ],
      "metadata": {
        "id": "7aB1yEMp2qut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4trBIN623uK",
        "outputId": "9b61724b-bd3b-45c5-c25e-935ad8c7abbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим, нам возвращаются обычные идентификаторы входа, идентификаторы типов токенов и маска внимания, а также необходимое нам сопоставление смещений и дополнительный ключ overflow_to_sample_mapping. Соответствующее значение мы будем использовать при токенизации нескольких текстов одновременно (что мы и должны делать, чтобы извлечь выгоду из того, что наш токенизатор основан на Rust). Поскольку один образец может давать несколько признаков, он сопоставляет каждый признак с примером, из которого он произошел. Поскольку здесь мы токенизировали только один пример, мы получим список 0:"
      ],
      "metadata": {
        "id": "0itGTQOk26gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"overflow_to_sample_mapping\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfcL1pl12--R",
        "outputId": "1a930945-080e-432a-8a19-6eae8f1356a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но если мы проведем токенизацию большего количества примеров, он станет эффективнее:"
      ],
      "metadata": {
        "id": "rPjVf94O3Ats"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    raw_datasets[\"train\"][2:6][\"question\"],\n",
        "    raw_datasets[\"train\"][2:6][\"context\"],\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqEN52yA26Jd",
        "outputId": "5ef6c321-9e18-4ae7-af05-7ed3c69b42bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 4 examples gave 19 features.\n",
            "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим, первые три примера (с индексами 2, 3 и 4 в обучающем наборе) дали по четыре признака, а последний пример (с индексом 5 в обучающем наборе) - 7 признаков."
      ],
      "metadata": {
        "id": "YPQBZABU3GzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта информация будет полезна для сопоставления каждого полученного признака с соответствующей меткой. Этими метками являются:\n",
        "\n",
        "* (0, 0), если ответ не находится в соответствующей области контекста\n",
        "* (start_position, end_position), если ответ находится в соответствующей области контекста, причем start_position - это индекс токена (во входных идентификаторах) в начале ответа, а end_position - индекс токена (во входных идентификаторах) в конце ответа"
      ],
      "metadata": {
        "id": "_QWslWhH3JDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы определить, какой из этих случаев имеет место, и, если нужно, позиции токенов, мы сначала находим индексы, с которых начинается и заканчивается контекст во входных идентификаторах. Для этого мы могли бы использовать идентификаторы типов токенов, но поскольку они не обязательно существуют для всех моделей (например, DistilBERT их не требует), вместо этого мы воспользуемся методом sequence_ids() из BatchEncoding, который возвращает наш токенизатор."
      ],
      "metadata": {
        "id": "TUWMzcwN3NRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получив индексы токенов, мы смотрим на соответствующие смещения, которые представляют собой кортежи из двух целых чисел, обозначающих промежуток символов внутри исходного контекста. Таким образом, мы можем определить, начинается ли фрагмент контекста в этом признаке после ответа или заканчивается до начала ответа (в этом случае метка будет (0, 0)). Если это не так, мы зацикливаемся, чтобы найти первый и последний токен ответа:"
      ],
      "metadata": {
        "id": "dqGY9NlK3Q8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "\n",
        "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
        "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
        "    answer = answers[sample_idx]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Найдём начало и конец контекста\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # Если ответ не полностью находится внутри контекста, меткой будет (0, 0)\n",
        "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        # В противном случае это начальная и конечная позиции токенов\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions.append(idx + 1)\n",
        "\n",
        "start_positions, end_positions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHBSBIVU3UYl",
        "outputId": "318e8097-86e0-461a-8ef7-7601a20ec551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
              " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте посмотрим на несколько результатов, чтобы убедиться в правильности нашего подхода. Для первого признака мы находим (83, 85) в качестве меток, поэтому давайте сравним теоретический ответ с декодированным диапазоном лексем с 83 по 85 (включительно):"
      ],
      "metadata": {
        "id": "KtGR5aD13kEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "start = start_positions[idx]\n",
        "end = end_positions[idx]\n",
        "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
        "\n",
        "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr_qajx23kap",
        "outputId": "f642a354-3095-483f-e372-a6a9f4a7b980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical answer: the Main Building, labels give: the Main Building\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, это совпадение! Теперь проверим индекс 4, где мы установили метки на (0, 0), что означает, что ответ не находится в фрагменте контекста этого признака:"
      ],
      "metadata": {
        "id": "ki-BUB9e3n-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 4\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
        "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0qNuLw_3oMg",
        "outputId": "3ab473f8-d148-4fd7-b9e0-6adfd13a3029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нет ответа в контексте."
      ],
      "metadata": {
        "id": "FLivZQYS3qjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда мы шаг за шагом разобрались с предварительной обработкой обучающих данных, мы можем сгруппировать их в функцию, которую будем применять ко всему датасету. Мы дополним каждый признак до максимальной длины, которую мы задали, поскольку большинство контекстов будут длинными (и соответствующие образцы будут разбиты на несколько признаков), поэтому применение динамического дополнения здесь не имеет реальной пользы:"
      ],
      "metadata": {
        "id": "4dNk5KGa3s0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_training_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Найдём начало и конец контекста\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # Если ответ не полностью находится внутри контекста, меткой будет (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # В противном случае это начальная и конечная позиции токенов\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "P36Vdot83zTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что мы определили две константы для определения максимальной длины и длины скользящего окна, а также добавили немного очистки перед токенизацией: некоторые вопросы в датасете SQuAD имеют лишние пробелы в начале и конце, которые ничего не добавляют, поэтому мы удалили эти лишние пробелы."
      ],
      "metadata": {
        "id": "APkw0apl37Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы применить эту функцию ко всему обучающему набору, мы используем метод Dataset.map() с флагом batched=True. Это необходимо, так как мы изменяем длину датасета (поскольку один пример может давать несколько обучающих признаков):"
      ],
      "metadata": {
        "id": "EP8O0_Ne4BNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = raw_datasets[\"train\"].map(\n",
        "    preprocess_training_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"train\"]), len(train_dataset)"
      ],
      "metadata": {
        "id": "UOvTj3WOc2wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим, предварительная обработка добавила около 1 000 признаков. Теперь наш обучающий набор готов к использованию. Перейдем к предварительной обработке валидационного набора."
      ],
      "metadata": {
        "id": "l9hjEAeR4NUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка валидационных данных"
      ],
      "metadata": {
        "id": "Si21dj_U4MeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предварительная обработка валидационных данных будет немного проще, поскольку нам не нужно генерировать метки. Настоящей радостью будет интерпретация прогнозов модели в диапазонах исходного контекста. Для этого нам нужно хранить как сопоставления смещений, так и способ сопоставления каждого созданного признака с оригинальным примером, из которого он взят. Поскольку в исходном датасете есть столбец ID, мы будем использовать этот ID."
      ],
      "metadata": {
        "id": "Tgt5tHXT4UAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нужно очистить сопоставления смещений. Они будут содержать смещения для вопроса и контекста, но на этапе постобработки у нас не будет возможности узнать, какая часть входных идентификаторов соответствует контексту, а какая - вопросу (метод sequence_ids(), который мы использовали, доступен только для выхода токенизатора). Поэтому мы установим смещения, соответствующие вопросу, в None:"
      ],
      "metadata": {
        "id": "T3NBhOeV4bOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "2QK64rl94ioi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = raw_datasets[\"validation\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"validation\"]), len(validation_dataset)"
      ],
      "metadata": {
        "id": "OH2I-tjuc4EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном случае мы добавили всего пару сотен примеров, поэтому контексты в валидационном датасете немного короче.\n",
        "\n",
        "Теперь, когда мы предварительно обработали все данные, можно приступать к обучению."
      ],
      "metadata": {
        "id": "IaGoFTvT4p3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning модели"
      ],
      "metadata": {
        "id": "j07nDkmK4sm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Постобработка"
      ],
      "metadata": {
        "id": "JSe8RxYW5Rfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель выведет логиты для начальной и конечной позиций ответа во входных идентификаторах.\n",
        "\n",
        "Чтобы ускорить процесс, мы также не будем оценивать все возможные пары (start_token, end_token), а только те, которые соответствуют наибольшим n_best логитам (при n_best=20). Так как мы пропустим softmax, эти оценки будут оценками логитов, и будут получены путем взятия суммы начального и конечного логитов (вместо произведения, по правилу \\(\\log(ab) = \\log(a) + \\log(b)\\)).\n",
        "\n",
        "Чтобы продемонстрировать все это, нам понадобятся некоторые прогнозы. Поскольку мы еще не обучили нашу модель, мы будем использовать модель по умолчанию для конвейера QA, чтобы сгенерировать несколько прогнозов на небольшой части набора для валидации. Мы можем использовать ту же функцию обработки, что и раньше; поскольку она опирается на глобальную константу tokenizer, нам просто нужно изменить этот объект на токенизатор модели, которую мы хотим временно использовать:"
      ],
      "metadata": {
        "id": "k4G-XhEj5i2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "eval_set = small_eval_set.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "owN4mySLc5AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда препроцессинг завершен, мы меняем токенизатор обратно на тот, который был выбран изначально:"
      ],
      "metadata": {
        "id": "QJQW04Sw5wyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "2h_P5XHF5yLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем мы удаляем из нашего eval_set столбцы, которые не ожидает модель, создаем батч со всей этой небольшой валидацией и пропускаем его через модель."
      ],
      "metadata": {
        "id": "DM9yEn945zmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(**batch)"
      ],
      "metadata": {
        "id": "7Yd67cRVc6Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку Trainer будет возвращать нам прогнозы в виде массивов NumPy, мы берем начальный и конечный логиты и конвертируем их в этот формат:"
      ],
      "metadata": {
        "id": "0nybj5dx6Am6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ],
      "metadata": {
        "id": "q3i3zidA6BFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь нам нужно найти спрогнозированный ответ для каждого примера в small_eval_set. Один пример может быть разбит на несколько признаков в eval_set, поэтому первым шагом будет сопоставление каждого примера в small_eval_set с соответствующими признаками в eval_set:"
      ],
      "metadata": {
        "id": "zOrGf_HC6Qjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_to_features = collections.defaultdict(list)\n",
        "for idx, feature in enumerate(eval_set):\n",
        "    example_to_features[feature[\"example_id\"]].append(idx)"
      ],
      "metadata": {
        "id": "mPY_asgN6SdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Имея это на руках, мы можем приступить к работе, итерируясь по всем примерам и, для каждого примера, по всем ассоциированным с ним признакам. Как мы уже говорили, мы рассмотрим оценки логитов для n_best начальных логитов и конечных логитов, исключая позиции, которые дают:\n",
        "\n",
        "Ответ, который не вписывается в контекст\n",
        "Ответ с отрицательной длиной\n",
        "Слишком длинный ответ (мы ограничиваем возможности по max_answer_length=30).\n",
        "После того как мы получили все возможные ответы для одного примера, мы просто выбираем тот, который имеет лучшую оценку логита:"
      ],
      "metadata": {
        "id": "FCmkNCP37lI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for example in small_eval_set:\n",
        "    example_id = example[\"id\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Пропускаем ответы, которые не полностью соответствуют контексту\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Пропускайте ответы, длина которых либо < 0, либо > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
      ],
      "metadata": {
        "id": "Y4hhUWkl7l-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Окончательный формат спрогнозированных ответов - это тот, который ожидает метрика, которую мы будем использовать. Как обычно, мы можем загрузить ее с помощью библиотеки Evaluate:"
      ],
      "metadata": {
        "id": "hKk_RN9m77Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"squad\")"
      ],
      "metadata": {
        "id": "qqM_675oc7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта метрика ожидает прогнозируемые ответы в формате, который мы видели выше (список словарей с одним ключом для идентификатора примера и одним ключом для прогнозируемого текста), и теоретические ответы в формате ниже (список словарей с одним ключом для идентификатора примера и одним ключом для возможных ответов):"
      ],
      "metadata": {
        "id": "8t2cgtu07-r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ],
      "metadata": {
        "id": "AZMSmRme7--q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы можем убедиться, что получаем разумные результаты, посмотрев на первый элемент обоих списков:"
      ],
      "metadata": {
        "id": "uTGubktr8BdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_answers[0])\n",
        "print(theoretical_answers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTmxZv8d8CR2",
        "outputId": "0cdc0c79-e46a-4f71-c486-7eb7563d999e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n",
            "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Не так уж плохо! Теперь давайте посмотрим на оценку, которую дает нам метрика:"
      ],
      "metadata": {
        "id": "ML8hs8tW8Dw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8YxIbNS8EL2",
        "outputId": "a646bb55-fa9b-41ac-bf39-038715d3552f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 83.0, 'f1': 88.25000000000004}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте поместим все, что мы только что сделали, в функцию compute_metrics(), которую мы будем использовать в Trainer. Обычно функция compute_metrics() получает только кортеж eval_preds с логитами и метками. Здесь нам понадобится немного больше, так как мы должны искать в датасете признаков смещения и в датасете примеров исходные контексты, поэтому мы не сможем использовать эту функцию для получения обычных результатов оценки во время обучения. Мы будем использовать ее только в конце обучения для проверки результатов.\n",
        "\n",
        "Функция compute_metrics() группирует те же шаги, что и до этого, только добавляется небольшая проверка на случай, если мы не найдем ни одного правильного ответа (в этом случае мы прогнозируем пустую строку)."
      ],
      "metadata": {
        "id": "ig2aYgac8Rna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(start_logits, end_logits, features, examples):\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Итерируемся по всем признакам, ассоциированным с этим примером\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Пропускаем ответы, которые не полностью соответствуют контексту\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Пропускайте ответы, длина которых либо < 0, либо > max_answer_length\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    answer = {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                    answers.append(answer)\n",
        "\n",
        "        # Выбираем ответ с лучшей оценкой\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ],
      "metadata": {
        "id": "Mp9IeV9p8W62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
      ],
      "metadata": {
        "id": "-nHI_ti7c9ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит отлично! Теперь давайте используем это для дообучения нашей модели."
      ],
      "metadata": {
        "id": "WXalrjNm8gZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дообучение модели"
      ],
      "metadata": {
        "id": "i7A557My8hNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы готовы к обучению нашей модели. Давайте сначала создадим ее, используя класс AutoModelForQuestionAnswering, как и раньше:"
      ],
      "metadata": {
        "id": "npFiBjJ58l66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "dPbMR2Fqc-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как обычно, мы получаем предупреждение о том, что некоторые веса не используются (веса головы предварительного обучения), а другие инициализируются случайным образом (веса головы ответов на вопросы). Вы уже должны были привыкнуть к этому, но это означает, что модель еще не готова к использованию и нуждается в дообучении - хорошо, что мы сейчас этим займемся!"
      ],
      "metadata": {
        "id": "SnGY-c2J8tJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можем определить наши TrainingArguments. Как мы уже говорили, когда определяли нашу функцию для вычисления метрики, мы не сможем сделать обычный цикл оценки из-за сигнатуры функции compute_metrics(). Мы могли бы написать собственный подкласс Trainer для этого, но это слишком длинно для данного раздела. Вместо этого мы будем оценивать модель только в конце обучения, а как проводить регулярную оценку, покажем ниже в разделе \"Пользовательский цикл обучения\"."
      ],
      "metadata": {
        "id": "jjLz31tI8yfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    \"bert-finetuned-squad\",\n",
        "    # evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "w5qbAL-r8tfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, мы просто передаем все в класс Trainer и запускаем обучение:"
      ],
      "metadata": {
        "id": "vguKWDBp9s6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8avuk_eiewo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда обучение завершено, мы можем наконец оценить нашу модель (и помолиться, что не потратили все это время вычислений впустую). Метод predict() функции Trainer вернет кортеж, где первыми элементами будут предсказания модели (здесь пара с начальным и конечным логитами). Мы отправляем его в нашу функцию compute_metrics():"
      ],
      "metadata": {
        "id": "5u_2AwYJ-Lcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, _, _ = trainer.predict(validation_dataset)\n",
        "start_logits, end_logits = predictions\n",
        "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
      ],
      "metadata": {
        "id": "SfBXvCcjc_16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Для сравнения, базовые показатели, указанные в статье BERT для этой модели, составляют 80,8 и 88,5, так что мы как раз там, где должны быть."
      ],
      "metadata": {
        "id": "K2OgZAT1-S-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, мы используем метод push_to_hub(), чтобы убедиться, что мы загрузили последнюю версию модели:"
      ],
      "metadata": {
        "id": "8siqvk_C-UG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\")"
      ],
      "metadata": {
        "id": "Rejp4vj4UAlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это возвращает URL только что выполненного коммита."
      ],
      "metadata": {
        "id": "Ovigua_w-XxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer также создает черновик карточки модели со всеми результатами оценки и загружает ее."
      ],
      "metadata": {
        "id": "_k7OvJlU-Ze8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Цикл обучения"
      ],
      "metadata": {
        "id": "Zuf6ROL6-cCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала нам нужно создать DataLoaderы из наших датасетов. Мы установим формат этих датасетов в \"torch\" и удалим столбцы в наборе валидации, которые не используются моделью. Затем мы можем использовать default_data_collator, предоставляемый Transformers, в качестве collate_fn и перемешаем обучающий набор, но не набор для валидации:"
      ],
      "metadata": {
        "id": "flg_2BPo-kpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(\"torch\")\n",
        "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "validation_set.set_format(\"torch\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "rRdVPiuY-lLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем мы реинстанцируем нашу модель, чтобы убедиться, что мы не продолжаем дообучение, а снова начинаем с предварительно обученной модели BERT:"
      ],
      "metadata": {
        "id": "6wBMiHfK-oQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "0uJXgdrpU2HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тогда нам понадобится оптимизатор. Как обычно, мы используем классический AdamW, который похож на Adam, но с исправлением в способе применения затухания веса:"
      ],
      "metadata": {
        "id": "wcK6m_sA-q6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "V4rzbotQ-rVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accelerator = Accelerator(fp16=True)\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "kusRHiDD--Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы можем использовать длину train_dataloader для вычисления количества шагов обучения только после того, как она пройдет через метод accelerator.prepare(). Мы используем тот же линейный график, что и в предыдущих разделах:"
      ],
      "metadata": {
        "id": "FOeWxKKGBGRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 2\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "gxSXgJfWBHln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы отправить нашу модель на Hub, нам нужно создать объект Repository в рабочей папке. Сначала войдите в Hub Hugging Face, если вы еще не вошли в него. Мы определим имя розитория по идентификатору модели, который мы хотим присвоить нашей модели (не стесняйтесь заменить repo_name на свое усмотрение; оно просто должно содержать ваше имя пользователя, что и делает функция get_full_repo_name()):"
      ],
      "metadata": {
        "id": "ryPiO5HTBKQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-finetuned-squad-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ],
      "metadata": {
        "id": "735_KZF2U4YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код ниже вызовет ошибку \"Repo not found\". В сообществе HuggingFace найдено следующее решение проблемы:\n",
        "* Перейдите к последней строке ошибки и найдите, какой репозиторий не найден.\n",
        "* Перейдите на страницу [Hugging Face - The AI community building the future](https://huggingface.co/new), чтобы создать репозиторий, который отсутствует.\n",
        "* Запустить код снова"
      ],
      "metadata": {
        "id": "Pnvb-XDYgtKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"bert-finetuned-squad-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ],
      "metadata": {
        "id": "JTjWm8nOUDiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Цикл обучения"
      ],
      "metadata": {
        "id": "VJUnE0EXBO9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы готовы написать полный цикл обучения. После определения прогресс-бара, чтобы следить за ходом обучения, цикл состоит из трех частей:\n",
        "\n",
        "Собственно обучение, которое представляет собой классическую итерацию по train_dataloader, прямой проход по модели, затем обратный проход и шаг оптимизатора.\n",
        "Оценка, в которой мы собираем все значения для start_logits и end_logits перед преобразованием их в массивы NumPy. После завершения цикла оценки мы объединяем все результаты. Обратите внимание, что нам нужно произвести усечение, потому что Accelerator может добавить несколько примеров в конце, чтобы убедиться, что у нас одинаковое количество примеров в каждом процессе.\n",
        "Сохранение и загрузка, где мы сначала сохраняем модель и токенизатор, а затем вызываем repo.push_to_hub(). Как и раньше, мы используем аргумент blocking=False, чтобы указать библиотеке 🤗 Hub на асинхронный процесс push. Таким образом, обучение продолжается нормально, а эта (длинная) инструкция выполняется в фоновом режиме.\n",
        "Вот полный код цикла обучения:"
      ],
      "metadata": {
        "id": "cw3gqk3OBTtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Обучение\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Оценка\n",
        "    model.eval()\n",
        "    start_logits = []\n",
        "    end_logits = []\n",
        "    accelerator.print(\"Evaluation!\")\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
        "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    start_logits = start_logits[: len(validation_dataset)]\n",
        "    end_logits = end_logits[: len(validation_dataset)]\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
        "    )\n",
        "    print(f\"epoch {epoch}:\", metrics)\n",
        "\n",
        "    # Сохранение и загрузка\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ],
      "metadata": {
        "id": "bbd80F9kdBYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Использование дообученной модели"
      ],
      "metadata": {
        "id": "KfmOh-JwBYf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы использовать ее локально в pipeline, нужно просто указать идентификатор модели:"
      ],
      "metadata": {
        "id": "s3U0asSnBb9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n",
        "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
        "\n",
        "context = \"\"\"\n",
        "Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZt5-NCvhlpZ",
        "outputId": "18d63014-7f4e-4fa5-fa3c-aea239b3b99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Список вопросов\n",
        "questions = [\n",
        "    \"Which deep learning libraries back Transformers?\",\n",
        "    \"How many popular deep learning libraries support Transformers?\",\n",
        "    \"Can you switch between libraries after training a model?\",\n",
        "    \"Name a library that integrates with Transformers other than PyTorch.\",\n",
        "    \"What is the main advantage of Transformers' library integration?\"\n",
        "]\n",
        "\n",
        "# Запуск модели для каждого вопроса\n",
        "for question in questions:\n",
        "    result = question_answerer(question=question, context=context)\n",
        "    print(f\"Вопрос: {question}\")\n",
        "    print(f\"Ответ: {result['answer']} (score: {result['score']:.4f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XqwUr_Qhmwc",
        "outputId": "147f4278-b678-4c88-944a-18e93d2ff6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: Which deep learning libraries back Transformers?\n",
            "Ответ: Jax, PyTorch and TensorFlow (score: 0.9982)\n",
            "\n",
            "Вопрос: How many popular deep learning libraries support Transformers?\n",
            "Ответ: three (score: 0.9982)\n",
            "\n",
            "Вопрос: Can you switch between libraries after training a model?\n",
            "Ответ: It's straightforward (score: 0.2549)\n",
            "\n",
            "Вопрос: Name a library that integrates with Transformers other than PyTorch.\n",
            "Ответ: TensorFlow (score: 0.9776)\n",
            "\n",
            "Вопрос: What is the main advantage of Transformers' library integration?\n",
            "Ответ: It's straightforward to train your models with one (score: 0.0449)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Наша модель работает так же хорошо, как и модель по умолчанию для этого конвейера!"
      ],
      "metadata": {
        "id": "0gbv8vsnBh4l"
      }
    }
  ]
}